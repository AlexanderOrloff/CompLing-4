{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа\n",
    "## Ранжирование с помощью ML\n",
    "\n",
    "\n",
    "![](https://avatars.mds.yandex.net/get-research/1677227/2a00000168a82fc9b0eac19e430b8454a656/orig)\n",
    "\n",
    "\n",
    "Одна из отличительных особенностей задачи ранжирования от классических задач машинного обучения заключается в том, что качество результата зависит не от предсказанных оценок релевантности, а от порядка следования документов в рамках конкретного запроса, т.е. важно не абсолютное значение релевантности (его достаточно трудно формализовать в виде числа), а то, более или менее релевантен документ, относительно других документов.\n",
    "\n",
    "### Подходы к решению задачи ранжирования\n",
    "Существуют 3 основных подхода к ранжированию, различие между которыми заключается в том, на какую функцию потерь они опираются:\n",
    "  \n",
    "1. **Поточечный подход (pointwise)**. В этом подходе предполагается, что каждой паре запрос-документ поставлена в соответствие численная оценка. Задача обучения ранжированию сводится к построению регрессии: для каждой отдельной пары запрос-документ необходимо предсказать её оценку.\n",
    "\n",
    "2. **Попарный подход (pairwise)**. В таком подходе обучение ранжированию сводится к построению бинарного классификатора, которому на вход поступают два документа, соответствующих одному и тому же запросу, и требуется определить, какой из них лучше. Другими словами, функция потерь штрафует модель, если отранжированная этой моделью пара документов оказалась в неправильном порядке.\n",
    "\n",
    "3. **Списочный подход (listwise)**. Его суть заключается в построении модели, на вход которой поступают сразу все документы, соответствующие запросу, а на выходе получается их перестановка.\n",
    "\n",
    "\n",
    "Будем использовать самый простой подход - поточечный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка качества\n",
    "\n",
    "Для оценивания качества ранжирования найденных документов в поиске традиционно используется метрика *DCG* ([Discounted Cumulative Gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)) и ее нормализованный вариант — *nDCG*, всегда принимающий значения от 0 до 1.\n",
    "\n",
    "Для одного запроса DCG считается следующим образом:\n",
    "$$ DCG(Q) = \\sum_{i=1}^{numpos}\\frac{(2^{rel_i} - 1)}{\\log_2(i+1)}, $$\n",
    "где\n",
    ">$numpos$ — количество документов в поисковой выдаче, среди которых мы оценимваем качество (например, в предудыщих заданиях *num_pos* был равен 5)  \n",
    "$rel_i$ — оценка релевантности документа, находящегося на i-той позиции   \n",
    "   \n",
    "\n",
    "Нормализованный вариант *nDCG* получается делением *DCG* на максимальное из его значений:\n",
    "\n",
    "$$nDCG = \\frac{DCG}{IDCG} \\in [0, 1].$$\n",
    "> *IDCG* — наибольшее из возможных значение *DCG* \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Чтобы оценить значение *nDCG* на выборке $Queries$ ($nDCG_{Queries}$) размера $N$, необходимо усреднить значение *nDCG* по всем запросам  выборки:\n",
    "$$nDCG_{Queries} = \\frac{1}{N}\\sum_{q \\in Queries}nDCG(q).$$\n",
    "\n",
    "Пример реализации метрик ранжирование на python можно найти [здесь](https://gist.github.com/mblondel/7337391)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Погнали\n",
    "###  **Задача: предсказать оценку релевантности для запросов тестового датасета**\n",
    "\n",
    "\n",
    "Мы будем работать на данных с конкурса [Интернет-математика 2009](https://academy.yandex.ru/events/data_analysis/grant2009/). По ссылке можно прочитать описание данных.      \n",
    "\n",
    "Данные\n",
    "> Данные разбиты на две выборки – обучающая выборка imat2009_learning.txt с известными оценками близости запроса и документа и тестовая выборка с неизвестными близостями imat2009_test.txt  \n",
    "\n",
    "Обучающая выборка\n",
    "> Данные для обучения содержат **97 290 строк**, которые соответствуют **9 124 запросам**  \n",
    "Каждая строка соответствует паре «запрос-документ»    \n",
    "\n",
    "Признаки\n",
    ">Каждой паре «запрос-документ» соответствуют значения **245 признаков**. Формат хранения feat_num:value. Если значение признака равно 0, то он опускается.     \n",
    "В комментариях в конце каждой строки указан **идентификатор запроса**.   \n",
    "Файл с обучающей выборкой содержит **оценку релевантности**, значения из диапазона **[0, 4]** (4 – «высокая релевантность», 0 – «нерелевантно»).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TRAIN DATA\n",
    "file_learning = 'imat2009-datasets/imat2009_learning.txt'\n",
    "\n",
    "# РЕП ОТКАЗАЛСЯ КУШАТЬ ДАТУ! \n",
    "\n",
    "with open(file_learning) as f:\n",
    "    train_data = f.readlines()\n",
    "    \n",
    "    \n",
    "# LOAD TEST DATA\n",
    "file_test = 'imat2009-datasets/imat2009_test.txt'\n",
    "\n",
    "with open(file_test) as f:\n",
    "    test_data = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97290, 115643)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Структура данных следующая - первый элемент в строке - это оценка близости запроса и документа, дальше идут признаки документа, а последний элемент строки - это id запроса:\n",
    "\n",
    "> RELEVANCE      feature:value feature:value ... feature:value     # QUERY_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 1:0.000023 7:0.704953 8:0.550315 9:0.032294 11:0.712631 14:0.015686 15:0.137255 16:0.302576 17:1.000000 18:0.996078 22:1.000000 23:1.000000 24:1.000000 27:0.700000 28:0.587629 29:0.999881 30:0.032294 34:0.000023 36:0.431373 37:0.002247 38:0.054902 41:1.000000 46:0.002247 50:0.032294 51:0.325613 52:0.056641 53:0.820677 54:0.388235 55:0.450980 56:0.312547 57:0.004672 59:1.000000 61:0.000023 65:1.000000 68:0.712195 69:0.001400 70:1.000000 71:0.001013 73:0.709459 74:0.560784 76:0.142857 77:0.360800 78:1.000000 79:1.000000 80:1.000000 82:0.000023 83:1.000000 85:0.996078 86:0.070588 87:1.000000 88:0.999797 92:1.000000 93:0.714286 95:0.039216 97:0.000023 98:0.356490 99:0.165041 102:1.000000 103:1.000000 104:1.000000 105:0.486275 108:0.152941 120:0.996078 121:0.676507 122:0.032294 126:0.712980 128:0.121569 129:0.609261 132:1.000000 134:0.109804 135:0.030535 140:0.002247 142:0.698039 144:0.248111 145:0.356490 146:1.000000 147:0.498039 148:0.125490 150:0.704953 151:1.000000 152:0.098039 154:0.676507 156:0.066667 157:0.001470 160:0.101961 162:0.302576 165:0.843126 166:0.400000 167:0.019608 168:0.056641 171:1.000000 172:0.857143 177:0.285714 178:0.588235 179:0.820677 180:0.032294 181:0.196491 182:0.729730 185:0.756863 192:1.000000 193:1.000000 197:0.032294 202:0.310127 203:0.001186 205:1.000000 206:0.999835 209:0.291145 210:0.980392 211:0.960784 212:0.032294 213:0.000023 214:1.000000 216:0.999998 217:0.146074 219:0.300000 222:0.666667 224:0.145098 227:0.007089 228:1.000000 229:1.000000 230:0.032294 232:1.000000 233:0.494217 236:0.032749 243:0.000023 244:1.000000 245:0.000023 # 3382\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В test_data все оценки релевантности скрыты, поскольку этот набор данных использовался для проверки качества работы алгоритма в конкурсе. Нам эти данные не нужны, дальше работаем только с **train_data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки качества будущей модели надо разбить обучающую выборку на обучение и валидацию в соотношении 70 / 30\n",
    "\n",
    "Внимание: разбивать необходимо **множество запросов QUERY_ID**, а не строчки датасета, чтобы в выборке находилась вся информация по запросу\n",
    "\n",
    "Для этого вам надо:\n",
    "1. собрать все запросы для каждого QUERY_ID\n",
    "\n",
    "```\n",
    "{\n",
    "query_id : [\n",
    "    RELEVANCE feature:value ... feature:value,\n",
    "    ...\n",
    "],\n",
    "...\n",
    "}\n",
    "```\n",
    "\n",
    "При этом я бы сразу собирала не сами данные, а номер строки в матрице данных\n",
    "```\n",
    "{\n",
    "query_id : [\n",
    "    line_num, line_num, ... line_num\n",
    "],\n",
    "...\n",
    "}\n",
    "```\n",
    "2. собрать матрицу данных, размер вектора равен числу признаков = 245\n",
    "```\n",
    "data = np.zeros((len(train_data), feats_num), dtype=np.float32) \n",
    "```\n",
    "\n",
    "3. собрать вектор с оценками релевантности, его размер равен размеру train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from math import floor\n",
    "\n",
    "labels = [] \n",
    "queries_lines_info = defaultdict(list) \n",
    "\n",
    "#train_data = train_data[:100]\n",
    "data = np.zeros((len(train_data), 245), dtype=np.float32) \n",
    "\n",
    "\n",
    "#YOUR CODE HERE\n",
    "for i, line in enumerate(train_data):\n",
    "    line_data = line.split(' ')\n",
    "    \n",
    "    label = floor(float(line_data[0])) #если так не делать, то из-за всяких полтора часть метрик выдаёт ошибку\n",
    "    query_id = int(line_data[-1])\n",
    "    \n",
    "    labels.append(label)\n",
    "    queries_lines_info[query_id].append(i)\n",
    "    \n",
    "    for feat_data in line_data[1:-2]:\n",
    "        feat_id, feat_value = feat_data.split(':')\n",
    "        feat_id = int(feat_id)-1\n",
    "        \n",
    "        data[i, feat_id] = feat_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {3382: [0, 1, 2, 3, 4, 5, 6],\n",
       "             11800: [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "             21991: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\n",
       "             19719: [33, 34, 35, 36],\n",
       "             11542: [37, 38, 39, 40, 41],\n",
       "             11546: [42, 43],\n",
       "             11547: [44, 45, 46, 47, 48, 49],\n",
       "             11544: [50,\n",
       "              51,\n",
       "              52,\n",
       "              53,\n",
       "              54,\n",
       "              55,\n",
       "              56,\n",
       "              57,\n",
       "              58,\n",
       "              59,\n",
       "              60,\n",
       "              61,\n",
       "              62,\n",
       "              63,\n",
       "              64,\n",
       "              65],\n",
       "             14472: [66, 67, 68, 69, 70, 71, 72, 73],\n",
       "             8090: [74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86],\n",
       "             11549: [87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98],\n",
       "             3483: [99]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_lines_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "assert data.shape == (len(train_data), 245)\n",
    "assert len(queries_lines_info.keys()) == 9124\n",
    "assert len(labels) == len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим запросы из *queries_lines_info.keys()* на обучающую *train_queries_ids* и валидационную выборки *test_queries_ids* (70/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#YOUR CODE HERE\n",
    "queries_ids = list(queries_lines_info.keys())\n",
    "train_queries_ids, test_queries_ids = train_test_split(queries_ids, test_size=0.3, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "assert len(train_queries_ids) / (len(train_queries_ids) + len(test_queries_ids)) == 0.6999123191582639"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > Теперь у нас есть:  \n",
    " 1) айдишники запросов для обучения и валидации **queries_id_train, queries_id_test**   \n",
    " 2) матрица данных **data**   \n",
    " 3) словарь **queries** с информацией о том, какие строчки в этой матрице соответствуют какому айдишнику  \n",
    " \n",
    " С помощью этих данных разделите матрицу data на матрицы **X_train, y_train, X_test, y_test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разбиваем номера строк исходной матрицы на train и test\n",
    "\n",
    "train_queries_lines_info = []\n",
    "test_queries_lines_info = []\n",
    "\n",
    "for query_id in train_queries_ids:\n",
    "    train_queries_lines_info += queries_lines_info[query_id]\n",
    "    \n",
    "for query_id in test_queries_ids:\n",
    "    test_queries_lines_info += queries_lines_info[query_id]\n",
    "    \n",
    "labels = np.array(labels)\n",
    "\n",
    "X_train, y_train = data[train_queries_lines_info], labels[train_queries_lines_info]\n",
    "X_test, y_test = data[test_queries_lines_info], labels[test_queries_lines_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries_lines_info = np.array(train_queries_lines_info)\n",
    "test_queries_lines_info = np.array(test_queries_lines_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поздравляю, если вы все сделали до этого моменты, вы восхитительны! \n",
    "\n",
    "Данные готовы, можно заряжать модели                                                           \n",
    "Для оценивания качества моделей используйте метрику nDCG, реализованную ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics\n",
    "\n",
    "def get_nDCG_score(queries, queries_lines_info, test_queries_lines_info, labels_true, labels_predicted):\n",
    "    nDCG_scores = [] # nDCG по каждому запросу\n",
    "    \n",
    "    for query in queries:\n",
    "        \n",
    "        query_lines = queries_lines_info[query]\n",
    "        query_lines_in_testdata = [np.where(test_queries_lines_info==line)[0][0] for line in query_lines]\n",
    "        \n",
    "        query_labels_true = labels[query_lines]\n",
    "        query_labels_pred = labels_predicted[query_lines_in_testdata]\n",
    "        \n",
    "        nDCG = metrics.ndcg_score(query_labels_true, query_labels_pred, k=10)\n",
    "        nDCG_scores.append(nDCG)\n",
    "        \n",
    "    nDCG_Queries = np.sum(nDCG_scores) / len(queries) # усредняем по всем запросам\n",
    "    return nDCG_Queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIT PREDICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Воспользовавшись известными вам техниками построения линейной регрессии, обучите модель, предсказывающую оценку асессора\n",
    "\n",
    "``` from sklearn.linear_model import LinearRegression``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "lin_reg_y_pred = lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "h = plt.hist(lin_reg_y_pred, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем качество модели по метрике **nDCG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8273441577972347"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_lin_reg = get_nDCG_score(\n",
    "    queries =  test_queries_ids, \n",
    "    queries_lines_info = queries_lines_info, \n",
    "    test_queries_lines_info = test_queries_lines_info, \n",
    "    labels_true = y_test, \n",
    "    labels_predicted = lin_reg_y_pred\n",
    ")\n",
    "\n",
    "score_lin_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь решим эту задачу не как регрессию, а как классификацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lin_svc = LinearSVC(max_iter = 100000)\n",
    "lin_svc.fit(X_train, y_train)\n",
    "\n",
    "lin_svc_pred = lin_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7851228182167391"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_svc_pred = get_nDCG_score(\n",
    "    queries =  test_queries_ids, \n",
    "    queries_lines_info = queries_lines_info, \n",
    "    test_queries_lines_info = test_queries_lines_info, \n",
    "    labels_true = y_test, \n",
    "    labels_predicted = lin_svc_pred\n",
    ")\n",
    "\n",
    "score_svc_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ранжируем с RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rand_tree = RandomForestRegressor()\n",
    "rand_tree.fit(X_train, y_train)\n",
    "\n",
    "rand_tree_predict = rand_tree.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8208874938408885"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_rand_tree = get_nDCG_score(\n",
    "    queries =  test_queries_ids, \n",
    "    queries_lines_info = queries_lines_info, \n",
    "    test_queries_lines_info = test_queries_lines_info, \n",
    "    labels_true = y_test, \n",
    "    labels_predicted = rand_tree_predict\n",
    ")\n",
    "\n",
    "score_rand_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ранжируем с XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "x = XGBClassifier()\n",
    "x.fit(X_train, y_train)\n",
    "\n",
    "x_predict = x.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7907175025324406"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_x = get_nDCG_score(\n",
    "    queries =  test_queries_ids, \n",
    "    queries_lines_info = queries_lines_info, \n",
    "    test_queries_lines_info = test_queries_lines_info, \n",
    "    labels_true = y_test, \n",
    "    labels_predicted = x_predict\n",
    ")\n",
    "\n",
    "score_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ранжируем с LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_bst = lgb.train({}, train_data)\n",
    "lightgbm_y_pred = lgb_bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8208874938408885"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_lgb = get_nDCG_score(\n",
    "    queries =  test_queries_ids, \n",
    "    queries_lines_info = queries_lines_info, \n",
    "    test_queries_lines_info = test_queries_lines_info, \n",
    "    labels_true = y_test, \n",
    "    labels_predicted = rand_tree_predict\n",
    ")\n",
    "\n",
    "score_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удивительным образом, решить задачу классификации как задачу регрессии дало наиболее высокий результат из всех имеющихся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
